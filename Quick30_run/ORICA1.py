import numpy as np
from scipy.stats import kurtosis
from sklearn.feature_selection import mutual_info_regression


class ORICA1:
    def __init__(self, n_components, learning_rate=0.001, ortho_every=10, 
                 use_rls_whitening=True, forgetting_factor=0.98, 
                 nonlinearity='gaussian', adaptive_ff='cooling', 
                 sample_rate=500, block_size=8, eval_convergence=True):
        """
        ORICA with RLS whitening support - å®Œæ•´ç‰ˆæœ¬
        
        Args:
            n_components: ç‹¬ç«‹æˆåˆ†æ•°é‡
            learning_rate: å­¦ä¹ ç‡
            ortho_every: æ¯éš”å¤šå°‘æ¬¡è¿­ä»£æ­£äº¤åŒ–
            use_rls_whitening: æ˜¯å¦ä½¿ç”¨RLSç™½åŒ–
            forgetting_factor: RLSé—å¿˜å› å­ (0 < Î» < 1)
            nonlinearity: éçº¿æ€§å‡½æ•°ç±»å‹ ('gaussian', 'tanh')
            adaptive_ff: é—å¿˜å› å­ç­–ç•¥ ('cooling', 'constant', 'adaptive')
            sample_rate: é‡‡æ ·ç‡
            block_size: å—å¤§å°
            eval_convergence: æ˜¯å¦è¯„ä¼°æ”¶æ•›æ€§
        """
        self.n_components = n_components
        self.learning_rate = learning_rate
        self.W = np.eye(n_components)  # è§£æ··çŸ©é˜µ
        self.mean = None
        self.whitening_matrix = None
        self.whitened = False
        self.update_count = 0
        self.ortho_every = ortho_every
        
        # RLSç™½åŒ–å‚æ•°
        self.use_rls_whitening = use_rls_whitening
        self.forgetting_factor = forgetting_factor
        self.nonlinearity = nonlinearity
        self.sample_rate = sample_rate
        self.block_size = block_size
        
        # è‡ªé€‚åº”é—å¿˜å› å­å‚æ•°
        self.adaptive_ff = adaptive_ff
        self.gamma = 0.6  # å†·å´ç­–ç•¥å‚æ•°
        self.lambda_0 = 0.995  # åˆå§‹é—å¿˜å› å­
        self.tau_const = 3  # å¸¸æ•°ç­–ç•¥å‚æ•°
        self.counter = 0  # æ—¶é—´è®¡æ•°å™¨
        
        # è‡ªé€‚åº”ç­–ç•¥å‚æ•°
        self.decay_rate_alpha = 0.02
        self.upper_bound_beta = 0.001
        self.trans_band_width = 1
        self.trans_band_center = 5
        self.min_norm_rn = None
        
        # æ”¶æ•›æ€§è¯„ä¼°
        self.eval_convergence = eval_convergence
        self.leaky_avg_delta = 0.01
        self.leaky_avg_delta_var = 1e-3
        self.Rn = None
        self.Var = None
        self.norm_rn = None
        
        # RLSç™½åŒ–ç›¸å…³å˜é‡
        if self.use_rls_whitening:
            self.C = None  # åæ–¹å·®çŸ©é˜µçš„é€†
            self.t = 0     # æ—¶é—´æ­¥è®¡æ•°å™¨

    def _center(self, X):
        """å»å‡å€¼"""
        if self.mean is None:
            self.mean = np.mean(X, axis=0)
        return X - self.mean

    def _whiten(self, X):
        """ä¼ ç»Ÿæ‰¹é‡ç™½åŒ– - ä½¿ç”¨ç‰¹å¾å€¼åˆ†è§£"""
        cov = np.cov(X, rowvar=False)
        d, E = np.linalg.eigh(cov)
        D_inv = np.diag(1.0 / np.sqrt(d + 1e-2))  # é˜²æ­¢é™¤0
        self.whitening_matrix = E @ D_inv @ E.T
        return X @ self.whitening_matrix.T

    def _rls_whiten_initialize(self, X):
        """åˆå§‹åŒ–RLSç™½åŒ–"""
        n_channels = X.shape[1]
        
        # åˆå§‹åŒ–åæ–¹å·®çŸ©é˜µçš„é€†ä¸ºå•ä½çŸ©é˜µ
        self.C = np.eye(n_channels)
        self.whitening_matrix = np.eye(n_channels)
        self.t = 0
        print(f"ğŸ”§ RLSç™½åŒ–åˆå§‹åŒ–: é€šé“æ•°={n_channels}, CçŸ©é˜µå½¢çŠ¶={self.C.shape}")

    def _rls_whiten_update(self, x_t):
        """
        RLSç™½åŒ–å•æ­¥æ›´æ–°
        
        Args:
            x_t: å•ä¸ªæ—¶é—´ç‚¹çš„æ•°æ® (n_channels, 1)
        """
        if self.C is None:
            raise ValueError("RLS whitening not initialized. Call initialize() first.")
        
        # æ£€æŸ¥ç»´åº¦åŒ¹é…
        expected_channels = self.C.shape[0]
        actual_channels = x_t.shape[0]
        
        if expected_channels != actual_channels:
            print(f"âš ï¸ RLSç™½åŒ–ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{expected_channels}é€šé“ï¼Œå®é™…{actual_channels}é€šé“")
            # é‡æ–°åˆå§‹åŒ–ä»¥åŒ¹é…æ–°çš„ç»´åº¦
            self.C = np.eye(actual_channels)
            self.whitening_matrix = np.eye(actual_channels)
            self.t = 0
            print(f"âœ… é‡æ–°åˆå§‹åŒ–RLSç™½åŒ–ï¼Œæ–°ç»´åº¦: {actual_channels}")
        
        # RLSæ›´æ–°è§„åˆ™
        lambda_t = self.forgetting_factor
        self.t += 1
        
        # è®¡ç®—å¢ç›Šå‘é‡
        k = self.C @ x_t
        denominator = lambda_t + x_t.T @ k
        k = k / denominator
        
        # æ›´æ–°åæ–¹å·®çŸ©é˜µçš„é€†
        self.C = (self.C - k @ x_t.T @ self.C) / lambda_t
        
        # æ›´æ–°ç™½åŒ–çŸ©é˜µ
        # ç™½åŒ–çŸ©é˜µæ˜¯åæ–¹å·®çŸ©é˜µé€†çš„å¹³æ–¹æ ¹
        eigenvals, eigenvecs = np.linalg.eigh(self.C)
        eigenvals = np.maximum(eigenvals, 1e-6)  # é˜²æ­¢è´Ÿå€¼
        self.whitening_matrix = eigenvecs @ np.diag(np.sqrt(eigenvals)) @ eigenvecs.T
        
        # è¿”å›ç™½åŒ–åçš„æ•°æ®
        return self.whitening_matrix @ x_t

    def _get_kurtosis_sign(self, y):
        """è®¡ç®—å³°åº¦ç¬¦å·"""
        k = kurtosis(y, axis=0, fisher=False)
        return k > 0  # True for supergaussian, False for subgaussian

    def _g(self, y):
        """
        æ”¹è¿›çš„éçº¿æ€§å‡½æ•°ï¼Œæ ¹æ®å³°åº¦ç¬¦å·é€‰æ‹©
        
        Args:
            y: è¾“å…¥ä¿¡å·
            
        Returns:
            g_y: éçº¿æ€§å‡½æ•°å€¼
            g_prime: å¯¼æ•°
        """
        if self.nonlinearity == 'gaussian':
            # é«˜æ–¯éçº¿æ€§å‡½æ•°
            g_y = y * np.exp(-0.5 * y**2)
            g_prime = (1 - y**2) * np.exp(-0.5 * y**2)
        elif self.nonlinearity == 'tanh':
            # åŒæ›²æ­£åˆ‡éçº¿æ€§å‡½æ•°
            g_y = np.tanh(y)
            g_prime = 1 - np.tanh(y)**2
        elif self.nonlinearity == 'adaptive':
            # è‡ªé€‚åº”éçº¿æ€§å‡½æ•° - æ ¹æ®å³°åº¦ç¬¦å·é€‰æ‹©
            kurt_sign = self._get_kurtosis_sign(y)
            
            g_y = np.zeros_like(y)
            g_prime = np.zeros_like(y)
            
            # Supergaussian components
            super_idx = kurt_sign
            g_y[super_idx] = -2 * np.tanh(y[super_idx])
            g_prime[super_idx] = -2 * (1 - np.tanh(y[super_idx])**2)
            
            # Subgaussian components  
            sub_idx = ~kurt_sign
            g_y[sub_idx] = np.tanh(y[sub_idx]) - y[sub_idx]
            g_prime[sub_idx] = 1 - np.tanh(y[sub_idx])**2 - 1
            
        else:
            raise ValueError(f"Unknown nonlinearity: {self.nonlinearity}")
        
        return g_y, g_prime

    def _compute_forgetting_factor(self, n_samples):
        """è®¡ç®—é—å¿˜å› å­"""
        if self.adaptive_ff == 'cooling':
            # å†·å´ç­–ç•¥: lambda = lambda_0 / t^gamma
            t_range = np.arange(self.counter + 1, self.counter + n_samples + 1)
            lambda_k = self.lambda_0 / (t_range ** self.gamma)
            lambda_const = 1 - np.exp(-1 / (self.tau_const * self.sample_rate))
            lambda_k = np.maximum(lambda_k, lambda_const)
            
        elif self.adaptive_ff == 'constant':
            # å¸¸æ•°ç­–ç•¥
            lambda_const = 1 - np.exp(-1 / (self.tau_const * self.sample_rate))
            lambda_k = np.full(n_samples, lambda_const)
            
        elif self.adaptive_ff == 'adaptive':
            # è‡ªé€‚åº”ç­–ç•¥
            if self.min_norm_rn is None:
                self.min_norm_rn = self.norm_rn if self.norm_rn is not None else 1.0
            self.min_norm_rn = max(min(self.min_norm_rn, self.norm_rn), 1)
            ratio = self.norm_rn / self.min_norm_rn
            lambda_k = self._adaptive_forgetting_factor(n_samples, ratio)
            
        else:
            lambda_k = np.full(n_samples, self.forgetting_factor)
        
        return lambda_k

    def _adaptive_forgetting_factor(self, n_samples, ratio):
        """è‡ªé€‚åº”é—å¿˜å› å­è®¡ç®—"""
        gain = self.upper_bound_beta * 0.5 * (1 + np.tanh((ratio - self.trans_band_center) / self.trans_band_width))
        
        lambda_k = np.zeros(n_samples)
        for n in range(n_samples):
            lambda_k[n] = ((1 + gain) ** (n + 1)) * self.forgetting_factor - \
                          self.decay_rate_alpha * (((1 + gain) ** (2 * n + 1)) - ((1 + gain) ** n)) / gain * (self.forgetting_factor ** 2)
        
        return lambda_k

    def _update_convergence_metrics(self, y, f, x_whitened, n_samples):
        """æ›´æ–°æ”¶æ•›æ€§æŒ‡æ ‡"""
        n_channels = y.shape[0]
        
        # æ¨¡å‹æ‹Ÿåˆåº¦
        model_fitness = np.eye(n_channels) + y @ f.T / n_samples
        
        # æ–¹å·®è®¡ç®—
        variance = x_whitened * x_whitened
        
        if self.Rn is None:
            self.Rn = model_fitness
            self.Var = np.sum(variance, axis=1) / (n_samples - 1)
        else:
            # æ³„æ¼å¹³å‡æ›´æ–°
            self.Rn = (1 - self.leaky_avg_delta) * self.Rn + self.leaky_avg_delta * model_fitness
            
            # æ–¹å·®æ›´æ–°
            decay_factors = (1 - self.leaky_avg_delta_var) ** np.arange(n_samples, 0, -1)
            self.Var = (1 - self.leaky_avg_delta_var) ** n_samples * self.Var + \
                       np.sum(self.leaky_avg_delta_var * variance * decay_factors.reshape(1, -1), axis=1)
        
        # éå¹³ç¨³æ€§æŒ‡æ•°
        self.norm_rn = np.linalg.norm(self.Rn, 'fro')

    def initialize(self, X_init):
        """åˆå§‹åŒ–ORICA"""
        # æ£€æŸ¥å¹¶è°ƒæ•´n_componentsä»¥åŒ¹é…æ•°æ®ç»´åº¦
        if X_init.shape[1] != self.n_components:
            print(f"âš ï¸ åˆå§‹åŒ–ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.n_components}é€šé“ï¼Œå®é™…{X_init.shape[1]}é€šé“")
            self.n_components = X_init.shape[1]
            # é‡æ–°åˆ›å»ºWçŸ©é˜µä»¥åŒ¹é…æ–°çš„ç»´åº¦
            self.W = np.eye(self.n_components)
            print(f"âœ… è°ƒæ•´n_componentsä¸º{self.n_components}")
        
        # å»å‡å€¼
        X_init = self._center(X_init)
        
        # ç™½åŒ–
        if self.use_rls_whitening:
            # ä½¿ç”¨RLSç™½åŒ–åˆå§‹åŒ–
            self._rls_whiten_initialize(X_init)
            # å¯¹åˆå§‹æ•°æ®è¿›è¡Œæ‰¹é‡ç™½åŒ–
            X_init = self._whiten(X_init)
        else:
            # ä½¿ç”¨ä¼ ç»Ÿæ‰¹é‡ç™½åŒ–
            X_init = self._whiten(X_init)
        
        self.whitened = True
        print(f"âœ… ORICAåˆå§‹åŒ–å®Œæˆ: n_components={self.n_components}, æ•°æ®å½¢çŠ¶={X_init.shape}")
        return X_init

    def partial_fit(self, x_t):
        """
        å•ä¸ªæ ·æœ¬åœ¨çº¿æ›´æ–°
        
        Args:
            x_t: å•ä¸ªæ—¶é—´ç‚¹çš„æ•°æ® (n_channels,)
        """
        x_t = x_t.reshape(-1, 1)  # ä» (25,) å˜ä¸º (25, 1)
        if not self.whitened:
            raise ValueError("Must call `initialize` with initial batch before `partial_fit`.")
        
        # æ£€æŸ¥è¾“å…¥ç»´åº¦æ˜¯å¦ä¸å½“å‰æ¨¡å‹åŒ¹é…
        if x_t.shape[0] != self.n_components:
            print(f"âš ï¸ partial_fitç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.n_components}é€šé“ï¼Œå®é™…{x_t.shape[0]}é€šé“")
            # é‡æ–°åˆå§‹åŒ–ä»¥åŒ¹é…æ–°çš„ç»´åº¦
            self.n_components = x_t.shape[0]
            self.W = np.eye(self.n_components)
            # é‡æ–°åˆå§‹åŒ–ç™½åŒ–
            if self.use_rls_whitening:
                self.C = np.eye(self.n_components)
                self.whitening_matrix = np.eye(self.n_components)
                self.t = 0
            else:
                self.whitening_matrix = np.eye(self.n_components)
            print(f"âœ… é‡æ–°åˆå§‹åŒ–ORICAï¼Œæ–°ç»´åº¦: {self.n_components}")
        
        # å»å‡å€¼
        if self.mean is not None and self.mean.shape[0] == x_t.shape[0]:
            x_t = x_t - self.mean.reshape(-1, 1)
        else:
            # å¦‚æœmeanç»´åº¦ä¸åŒ¹é…ï¼Œé‡æ–°è®¡ç®—
            print(f"âš ï¸ å‡å€¼ç»´åº¦ä¸åŒ¹é…ï¼Œé‡æ–°è®¡ç®—")
            self.mean = np.zeros(x_t.shape[0])
        
        # ç™½åŒ–
        if self.use_rls_whitening:
            # RLSç™½åŒ–æ›´æ–°
            x_t_whitened = self._rls_whiten_update(x_t)
        else:
            # ä¼ ç»Ÿç™½åŒ–
            x_t_whitened = self.whitening_matrix @ x_t
        
        # ICAæ›´æ–°
        y_t = self.W @ x_t_whitened
        g_y, _ = self._g(y_t)
        
        # ORICAæ›´æ–°è§„åˆ™
        I = np.eye(self.n_components)
        delta_W = self.learning_rate * ((I - g_y @ y_t.T) @ self.W)
        self.W += delta_W

        # æ­£äº¤åŒ–
        self.update_count += 1
        if self.update_count % self.ortho_every == 0:
            U, _, Vt = np.linalg.svd(self.W)
            self.W = U @ Vt

        return y_t.ravel()

    def partial_fit_block(self, X_block, time_perm=True):
        """
        å—æ›´æ–° - æ›´æ¥è¿‘MATLABå®ç°
        
        Args:
            X_block: æ•°æ®å— (n_channels, n_samples)
            time_perm: æ˜¯å¦è¿›è¡Œæ—¶é—´æ’åˆ—
        """
        if not self.whitened:
            raise ValueError("Must call `initialize` first.")
        
        n_channels, n_samples = X_block.shape
        
        # æ£€æŸ¥ç»´åº¦åŒ¹é…
        if n_channels != self.n_components:
            print(f"âš ï¸ å—æ›´æ–°ç»´åº¦ä¸åŒ¹é…: æœŸæœ›{self.n_components}é€šé“ï¼Œå®é™…{n_channels}é€šé“")
            self.n_components = n_channels
            self.W = np.eye(self.n_components)
            if self.use_rls_whitening:
                self.C = np.eye(self.n_components)
                self.whitening_matrix = np.eye(self.n_components)
                self.t = 0
            else:
                self.whitening_matrix = np.eye(self.n_components)
            print(f"âœ… é‡æ–°åˆå§‹åŒ–ORICAï¼Œæ–°ç»´åº¦: {self.n_components}")
        
        # å»å‡å€¼
        if self.mean is not None and self.mean.shape[0] == n_channels:
            X_block = X_block - self.mean.reshape(-1, 1)
        else:
            print(f"âš ï¸ å‡å€¼ç»´åº¦ä¸åŒ¹é…ï¼Œé‡æ–°è®¡ç®—")
            self.mean = np.zeros(n_channels)
        
        # ç™½åŒ–
        if self.use_rls_whitening:
            X_whitened = self.whitening_matrix @ X_block
        else:
            X_whitened = self.whitening_matrix @ X_block
        
        # æ—¶é—´æ’åˆ—
        if time_perm:
            perm_idx = np.random.permutation(n_samples)
            X_whitened = X_whitened[:, perm_idx]
        
        # è®¡ç®—æºæ¿€æ´»
        y = self.W @ X_whitened
        
        # éçº¿æ€§å‡½æ•°
        f, _ = self._g(y)
        
        # è®¡ç®—é—å¿˜å› å­
        lambda_k = self._compute_forgetting_factor(n_samples)
        
        # è®¡ç®—æ”¶æ•›æ€§æŒ‡æ ‡
        if self.eval_convergence:
            self._update_convergence_metrics(y, f, X_whitened, n_samples)
        
        # ORICAå—æ›´æ–°è§„åˆ™
        lambda_prod = np.prod(1.0 / (1.0 - lambda_k))
        Q = 1.0 + lambda_k * (np.sum(f * y, axis=0) - 1.0)
        
        # æ›´æ–°æƒé‡çŸ©é˜µ
        delta_W = np.zeros_like(self.W)
        for i in range(n_samples):
            delta_W += (y[:, i:i+1] * (lambda_k[i] / Q[i]) * f[:, i:i+1].T) @ self.W
        
        self.W = lambda_prod * (self.W - delta_W)
        
        # æ­£äº¤åŒ–
        U, _, Vt = np.linalg.svd(self.W)
        self.W = U @ Vt
        
        self.counter += n_samples
        return y

    def fit_online(self, X_stream):
        """
        åœ¨çº¿æ‹Ÿåˆ - å¤„ç†æ•°æ®æµ
        
        Args:
            X_stream: æ•°æ®æµ (n_samples, n_channels)
        """
        results = []
        
        # åˆ†å—å¤„ç†
        for i in range(0, len(X_stream), self.block_size):
            block = X_stream[i:i+self.block_size].T  # è½¬ç½®ä¸º (n_channels, n_samples)
            y = self.partial_fit_block(block)
            results.append(y.T)  # è½¬ç½®å› (n_samples, n_channels)
        
        return np.vstack(results)

    def transform(self, X):
        """å˜æ¢æ•°æ®"""
        if not self.whitened:
            raise ValueError("Model must be initialized first with `initialize()`.")
        
        # å»å‡å€¼
        X = X - self.mean
        
        # ç™½åŒ–
        if self.use_rls_whitening:
            # ä½¿ç”¨å½“å‰çš„ç™½åŒ–çŸ©é˜µ
            X_whitened = X @ self.whitening_matrix.T
        else:
            # ä¼ ç»Ÿç™½åŒ–
            X_whitened = X @ self.whitening_matrix.T
        
        # ICAå˜æ¢
        Y = (self.W @ X_whitened.T).T
        return Y

    def inverse_transform(self, Y):
        """é€†å˜æ¢"""
        Xw = np.linalg.pinv(self.W) @ Y.T
        X = Xw.T @ np.linalg.pinv(self.whitening_matrix).T + self.mean
        return X

    def get_W(self):
        """è·å–è§£æ··çŸ©é˜µ"""
        return self.W

    def get_whitening_matrix(self):
        """è·å–ç™½åŒ–çŸ©é˜µ"""
        return self.whitening_matrix

    def get_convergence_metrics(self):
        """è·å–æ”¶æ•›æ€§æŒ‡æ ‡"""
        return {
            'norm_rn': self.norm_rn,
            'Rn': self.Rn,
            'Var': self.Var
        }

    def evaluate_separation(self, Y):
        """è¯„ä¼°åˆ†ç¦»æ•ˆæœ - ä½¿ç”¨å³°åº¦"""
        k = kurtosis(Y, axis=0, fisher=False)
        return k

    def rank_components_by_kurtosis(self, Y):
        """æŒ‰å³°åº¦æ’åºæˆåˆ†"""
        k = self.evaluate_separation(Y)
        indices = np.argsort(-np.abs(k))
        return indices, k

    def calc_mutual_info_matrix(self, sources):
        """è®¡ç®—äº’ä¿¡æ¯çŸ©é˜µ"""
        n = sources.shape[0]
        MI = np.zeros((n, n))
        for i in range(n):
            for j in range(n):
                if i != j:
                    MI[i, j] = mutual_info_regression(
                        sources[i, :].reshape(-1, 1), sources[j, :]
                    )[0]
        return MI

    def set_nonlinearity(self, nonlinearity):
        """è®¾ç½®éçº¿æ€§å‡½æ•°ç±»å‹"""
        if nonlinearity not in ['gaussian', 'tanh', 'adaptive']:
            raise ValueError("nonlinearity must be 'gaussian', 'tanh', or 'adaptive'")
        self.nonlinearity = nonlinearity

    def set_forgetting_factor(self, forgetting_factor):
        """è®¾ç½®RLSé—å¿˜å› å­"""
        if not (0 < forgetting_factor < 1):
            raise ValueError("forgetting_factor must be between 0 and 1")
        self.forgetting_factor = forgetting_factor

    def set_adaptive_ff(self, adaptive_ff):
        """è®¾ç½®è‡ªé€‚åº”é—å¿˜å› å­ç­–ç•¥"""
        if adaptive_ff not in ['cooling', 'constant', 'adaptive']:
            raise ValueError("adaptive_ff must be 'cooling', 'constant', or 'adaptive'")
        self.adaptive_ff = adaptive_ff


