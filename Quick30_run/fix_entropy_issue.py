#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ä¿®å¤ç†µè®¡ç®—é—®é¢˜å¹¶æ”¹è¿›ORICA
"""

import numpy as np
from scipy.stats import kurtosis, entropy
import matplotlib.pyplot as plt

def fix_entropy_calculation(y_t):
    """ä¿®å¤ç†µè®¡ç®—é—®é¢˜"""
    # æ–¹æ³•1: ä½¿ç”¨scipyçš„entropyå‡½æ•°
    hist, bin_edges = np.histogram(y_t, bins=20, density=True)
    # ç§»é™¤é›¶æ¦‚ç‡ï¼Œé¿å…log(0)
    hist = hist[hist > 1e-10]
    if len(hist) > 0:
        entropy_scipy = entropy(hist)
    else:
        entropy_scipy = 0
    
    # æ–¹æ³•2: ä½¿ç”¨å·®åˆ†ç†µä¼°è®¡
    # å¯¹äºè¿ç»­å˜é‡ï¼Œä½¿ç”¨å·®åˆ†ç†µ
    var_y = np.var(y_t)
    if var_y > 0:
        # å‡è®¾é«˜æ–¯åˆ†å¸ƒçš„å·®åˆ†ç†µ
        diff_entropy = 0.5 * np.log(2 * np.pi * np.e * var_y)
    else:
        diff_entropy = 0
    
    # æ–¹æ³•3: ä½¿ç”¨k-è¿‘é‚»ç†µä¼°è®¡
    from sklearn.neighbors import NearestNeighbors
    if len(y_t) > 10:
        y_reshaped = y_t.reshape(-1, 1)
        nbrs = NearestNeighbors(n_neighbors=2).fit(y_reshaped)
        distances, _ = nbrs.kneighbors(y_reshaped)
        # ä½¿ç”¨æœ€è¿‘é‚»è·ç¦»ä¼°è®¡ç†µ
        knn_entropy = np.mean(np.log(distances[:, 1] + 1e-10))
    else:
        knn_entropy = 0
    
    return {
        'scipy_entropy': entropy_scipy,
        'diff_entropy': diff_entropy,
        'knn_entropy': knn_entropy
    }

def analyze_entropy_issue():
    """åˆ†æç†µè®¡ç®—é—®é¢˜"""
    print("=== åˆ†æç†µè®¡ç®—é—®é¢˜ ===")
    
    # ç”Ÿæˆä¸åŒåˆ†å¸ƒçš„æµ‹è¯•æ•°æ®
    n_samples = 1000
    
    # 1. é«˜æ–¯åˆ†å¸ƒ
    gaussian_data = np.random.randn(n_samples)
    
    # 2. å‡åŒ€åˆ†å¸ƒ
    uniform_data = np.random.uniform(-2, 2, n_samples)
    
    # 3. æ‹‰æ™®æ‹‰æ–¯åˆ†å¸ƒ
    laplace_data = np.random.laplace(0, 1, n_samples)
    
    # 4. æ··åˆåˆ†å¸ƒï¼ˆæ¨¡æ‹ŸICAè¾“å‡ºï¼‰
    mixed_data = np.random.randn(n_samples)
    mixed_data[:500] = np.sign(mixed_data[:500]) * np.abs(mixed_data[:500])**1.5
    
    test_data = {
        'Gaussian': gaussian_data,
        'Uniform': uniform_data,
        'Laplace': laplace_data,
        'Mixed': mixed_data
    }
    
    print("\nğŸ“Š ä¸åŒåˆ†å¸ƒçš„ç†µè®¡ç®—ç»“æœ:")
    for name, data in test_data.items():
        entropy_results = fix_entropy_calculation(data)
        print(f"\n{name}åˆ†å¸ƒ:")
        print(f"  Scipyç†µ: {entropy_results['scipy_entropy']:.3f}")
        print(f"  å·®åˆ†ç†µ: {entropy_results['diff_entropy']:.3f}")
        print(f"  KNNç†µ: {entropy_results['knn_entropy']:.3f}")
        print(f"  æ–¹å·®: {np.var(data):.3f}")
        print(f"  å³°åº¦: {kurtosis(data):.3f}")
    
    # ç»˜åˆ¶åˆ†å¸ƒå›¾
    plt.figure(figsize=(15, 10))
    
    for i, (name, data) in enumerate(test_data.items()):
        plt.subplot(2, 2, i+1)
        plt.hist(data, bins=30, alpha=0.7, density=True)
        plt.title(f'{name}åˆ†å¸ƒ')
        plt.xlabel('å€¼')
        plt.ylabel('å¯†åº¦')
        plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('entropy_analysis.png', dpi=300, bbox_inches='tight')
    plt.show()

def improved_orica_entropy():
    """æ”¹è¿›çš„ORICAç†µè®¡ç®—"""
    class ORICAEntropyFixed:
        def __init__(self, n_components, learning_rate=0.01, ortho_every=10):
            self.n_components = n_components
            self.learning_rate = learning_rate
            self.ortho_every = ortho_every
            self.W = np.eye(n_components)
            self.mean = None
            self.whitening_matrix = None
            self.whitened = False
            self.update_count = 0
            
            # ç†µè®¡ç®—å†å²
            self.entropy_history = []
            self.spatial_concentration_history = []
        
        def _compute_entropy_fixed(self, y_t):
            """ä¿®å¤çš„ç†µè®¡ç®—"""
            # ä½¿ç”¨å·®åˆ†ç†µä¼°è®¡
            var_y = np.var(y_t)
            if var_y > 1e-10:
                # å·®åˆ†ç†µ
                diff_entropy = 0.5 * np.log(2 * np.pi * np.e * var_y)
                
                # æ·»åŠ éé«˜æ–¯æ€§ä¿®æ­£
                kurt = kurtosis(y_t)
                non_gaussian_correction = 0.1 * np.abs(kurt)
                
                corrected_entropy = diff_entropy + non_gaussian_correction
            else:
                corrected_entropy = 0
            
            return corrected_entropy
        
        def _compute_spatial_concentration(self):
            """è®¡ç®—ç©ºé—´é›†ä¸­åº¦"""
            if not hasattr(self, 'W') or self.W is None:
                return 1.0
            
            # è®¡ç®—WçŸ©é˜µçš„ç©ºé—´é›†ä¸­åº¦
            W_abs = np.abs(self.W)
            max_weights = np.max(W_abs, axis=1)
            mean_weights = np.mean(W_abs, axis=1)
            
            # é¿å…é™¤é›¶
            mean_weights = np.maximum(mean_weights, 1e-10)
            concentration = np.mean(max_weights / mean_weights)
            
            return concentration
        
        def initialize(self, X_init):
            """åˆå§‹åŒ–"""
            self.mean = np.mean(X_init, axis=0)
            X_centered = X_init - self.mean
            
            # ç™½åŒ–
            cov = np.cov(X_centered, rowvar=False)
            d, E = np.linalg.eigh(cov)
            D_inv = np.diag(1.0 / np.sqrt(d + 1e-8))
            self.whitening_matrix = E @ D_inv @ E.T
            
            self.whitened = True
            print("âœ… åˆå§‹åŒ–å®Œæˆ")
        
        def partial_fit(self, x_t):
            """å•ä¸ªæ ·æœ¬æ›´æ–°"""
            if not self.whitened:
                raise ValueError("Must initialize first")
            
            # å»å‡å€¼
            x_t = x_t - self.mean
            
            # ç™½åŒ–
            x_whitened = self.whitening_matrix @ x_t
            
            # ICAæ›´æ–°
            y_t = self.W @ x_whitened
            
            # éçº¿æ€§å‡½æ•°
            g_y = y_t * np.exp(-y_t**2/2)
            
            # æ›´æ–°è§„åˆ™
            I = np.eye(self.n_components)
            delta_W = self.learning_rate * ((I - g_y @ y_t.T) @ self.W)
            self.W += delta_W
            
            # æ­£äº¤åŒ–
            self.update_count += 1
            if self.update_count % self.ortho_every == 0:
                U, _, Vt = np.linalg.svd(self.W)
                self.W = U @ Vt
                
                # è®¡ç®—å¹¶è®°å½•æŒ‡æ ‡
                entropy_val = self._compute_entropy_fixed(y_t)
                spatial_conc = self._compute_spatial_concentration()
                
                self.entropy_history.append(entropy_val)
                self.spatial_concentration_history.append(spatial_conc)
                
                # æ‰“å°ç›‘æ§ä¿¡æ¯
                if self.update_count % 100 == 0:
                    print(f"  æ­¥éª¤ {self.update_count}: ç†µ={entropy_val:.3f}, "
                          f"ç©ºé—´é›†ä¸­åº¦={spatial_conc:.3f}")
            
            return y_t
        
        def get_metrics(self):
            """è·å–å½“å‰æŒ‡æ ‡"""
            if len(self.entropy_history) == 0:
                return None
            
            return {
                'entropy': self.entropy_history[-1],
                'spatial_concentration': self.spatial_concentration_history[-1],
                'entropy_trend': np.mean(self.entropy_history[-10:]) if len(self.entropy_history) >= 10 else 0,
                'concentration_trend': np.mean(self.spatial_concentration_history[-10:]) if len(self.spatial_concentration_history) >= 10 else 0
            }
    
    return ORICAEntropyFixed

def test_improved_entropy():
    """æµ‹è¯•æ”¹è¿›çš„ç†µè®¡ç®—"""
    print("\n=== æµ‹è¯•æ”¹è¿›çš„ç†µè®¡ç®— ===")
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    n_channels = 25
    n_samples = 10000
    
    # ç”Ÿæˆæ··åˆä¿¡å·
    sources = np.random.randn(n_channels, n_samples)
    sources[0, :] = np.sign(sources[0, :]) * np.abs(sources[0, :])**1.5
    sources[1, :] = np.tanh(sources[1, :])
    
    # ç”Ÿæˆæ··åˆçŸ©é˜µ
    A = np.random.randn(n_channels, n_channels)
    A = A / np.linalg.norm(A, axis=0)
    
    # æ··åˆä¿¡å·
    X = A @ sources
    
    # åˆ›å»ºæ”¹è¿›çš„ORICAå®ä¾‹
    ORICAFixed = improved_orica_entropy()
    orica = ORICAFixed(
        n_components=n_channels,
        learning_rate=0.01,
        ortho_every=10
    )
    
    # åˆå§‹åŒ–
    orica.initialize(X[:1000].T)
    
    # åœ¨çº¿å­¦ä¹ 
    print("å¼€å§‹åœ¨çº¿å­¦ä¹ ...")
    for i in range(1000, len(X), 100):
        batch = X[:, i:i+100].T
        
        for x_t in batch:
            result = orica.partial_fit(x_t)
        
        if i % 2000 == 0:
            metrics = orica.get_metrics()
            if metrics:
                print(f"  æ­¥éª¤ {i}: ç†µ={metrics['entropy']:.3f}, "
                      f"ç©ºé—´é›†ä¸­åº¦={metrics['spatial_concentration']:.3f}")
    
    print("âœ… å­¦ä¹ å®Œæˆ")
    
    # åˆ†ææœ€ç»ˆç»“æœ
    final_metrics = orica.get_metrics()
    if final_metrics:
        print(f"\nğŸ“Š æœ€ç»ˆæŒ‡æ ‡:")
        print(f"  ç†µ: {final_metrics['entropy']:.3f}")
        print(f"  ç©ºé—´é›†ä¸­åº¦: {final_metrics['spatial_concentration']:.3f}")
        print(f"  ç†µè¶‹åŠ¿: {final_metrics['entropy_trend']:.3f}")
        print(f"  é›†ä¸­åº¦è¶‹åŠ¿: {final_metrics['concentration_trend']:.3f}")
    
    # ç»˜åˆ¶ç»“æœ
    plt.figure(figsize=(15, 5))
    
    # ç†µå˜åŒ–
    plt.subplot(1, 3, 1)
    plt.plot(orica.entropy_history)
    plt.title('ç†µå˜åŒ–')
    plt.xlabel('æ›´æ–°æ­¥éª¤')
    plt.ylabel('ç†µ')
    plt.grid(True)
    
    # ç©ºé—´é›†ä¸­åº¦å˜åŒ–
    plt.subplot(1, 3, 2)
    plt.plot(orica.spatial_concentration_history)
    plt.title('ç©ºé—´é›†ä¸­åº¦å˜åŒ–')
    plt.xlabel('æ›´æ–°æ­¥éª¤')
    plt.ylabel('ç©ºé—´é›†ä¸­åº¦')
    plt.grid(True)
    
    # ç†µåˆ†å¸ƒ
    plt.subplot(1, 3, 3)
    plt.hist(orica.entropy_history, bins=20, alpha=0.7)
    plt.title('ç†µåˆ†å¸ƒ')
    plt.xlabel('ç†µå€¼')
    plt.ylabel('é¢‘æ¬¡')
    plt.grid(True)
    
    plt.tight_layout()
    plt.savefig('improved_entropy_results.png', dpi=300, bbox_inches='tight')
    plt.show()

if __name__ == "__main__":
    # åˆ†æç†µè®¡ç®—é—®é¢˜
    analyze_entropy_issue()
    
    # æµ‹è¯•æ”¹è¿›çš„ç†µè®¡ç®—
    test_improved_entropy() 