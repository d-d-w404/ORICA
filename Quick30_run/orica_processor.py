from ORICA import ORICA
from ORICA_none_rls import ORICAX
from ORICA1 import ORICA1
from ORICA_enhanced import ORICAW
#from ORICA_REST import ORICAZ
from ORICA_REST_new import ORICAZ
from ORICA_final import ORICA_final
# from ORICA_old import ORICA
import numpy as np
from scipy.signal import welch
import mne
from mne_icalabel import label_components
from mne.preprocessing import ICA
import matplotlib

from scipy.stats import kurtosis
from sklearn.metrics import mutual_info_score
import numpy as np
import json
import os
from datetime import datetime
import csv
# æ›´å®‰å…¨çš„GUIåç«¯è®¾ç½®
try:
    matplotlib.use('Agg')  # ä¼˜å…ˆä½¿ç”¨éGUIåç«¯
except:
    try:
        matplotlib.use('TkAgg')  # å¤‡ç”¨æ–¹æ¡ˆ
    except:
        matplotlib.use('Qt5Agg')  # æœ€åå¤‡ç”¨æ–¹æ¡ˆ

import matplotlib.pyplot as plt
from scipy.stats import kurtosis
import seaborn as sns


class ORICAProcessor:
    def __init__(self, n_components=None, max_samples=10000, srate=None):
        self.n_components = n_components
        self.max_samples = max_samples
        self.srate = srate  # âœ… ä¿å­˜é‡‡æ ·ç‡
        self.ica = None
        self.icax = None
        self.data_buffer = None
        self.eog_indices = []  # indices of components identified as eye artifacts
        self.artifact = []

        self.sorted_W = None
        self.sorted_idx = None





    def assess_orica_success(self,k_values, threshold_strong=5.0, threshold_good=3.0, verbose=True):
        k = np.array(k_values)
        count_strong = np.sum(k > threshold_strong)
        count_good = np.sum((k > threshold_good) & (k <= threshold_strong))
        mean_k = np.mean(k)
        std_k = np.std(k)
        max_k = np.max(k)

        if verbose:
            print(f"ğŸ” ORICA å…¨å±€è¯„ä¼°ï¼š")
            print(f"  â–¶ï¸ æå¼ºéé«˜æ–¯æºæ•° (> {threshold_strong}): {count_strong}")
            print(f"  â–¶ï¸ å¯æ¥å—è„‘æºæˆåˆ†æ•° ({threshold_good}~{threshold_strong}): {count_good}")
            print(f"  â–¶ï¸ å³°åº¦å‡å€¼: {mean_k:.2f}, æ ‡å‡†å·®: {std_k:.2f}, æœ€å¤§å³°åº¦: {max_k:.2f}")

        # åˆ¤æ–­æˆåŠŸæ¡ä»¶ï¼šè‡³å°‘ä¸€ä¸ª strongï¼Œæˆ–å¤šä¸ª good ä¸”å³°åº¦åˆ†å¸ƒæ‹‰å¼€
        if count_strong >= 1:
            return True
        elif count_good >= 2 and std_k > 0.5:
            return True
        else:
            return False




    def evaluate_orica_sources(self, sources, n_bins=10):
        from scipy.stats import kurtosis
        from sklearn.metrics import mutual_info_score
        import numpy as np

        # å³­åº¦
        kurt_vals = kurtosis(sources, axis=1, fisher=True)
        # print("ã€ORICAè¯„ä¼°ã€‘å„ICçš„å³­åº¦ï¼ˆè¶Šå¤§è¶Šéé«˜æ–¯ï¼Œåˆ†ç¦»æ•ˆæœè¶Šå¥½ï¼‰:")
        # print(kurt_vals)
        kurtosis_mean = np.mean(np.abs(kurt_vals))
        print(f"kurtosis mean: {kurtosis_mean:.3f}")

        # äº’ä¿¡æ¯ï¼ˆå…ˆç¦»æ•£åŒ–ï¼‰
        n = sources.shape[0]
        mi_matrix = np.zeros((n, n))
        # ç¦»æ•£åŒ–
        digitized = np.array([np.digitize(s, np.histogram(s, bins=n_bins)[1]) for s in sources])
        for i in range(n):
            for j in range(n):
                if i != j:
                    mi_matrix[i, j] = mutual_info_score(digitized[i], digitized[j])
        #print("ã€ORICAè¯„ä¼°ã€‘ICä¹‹é—´çš„äº’ä¿¡æ¯çŸ©é˜µï¼ˆè¶Šæ¥è¿‘0è¶Šç‹¬ç«‹ï¼‰:")
        np.set_printoptions(precision=3, suppress=True)
        #print(mi_matrix)
        # åªç»Ÿè®¡éå¯¹è§’çº¿å…ƒç´ çš„å‡å€¼
        mi_mean = np.sum(mi_matrix) / (n * (n - 1))
        print(f"MI mean: {mi_mean:.3f}")

        # ä¿å­˜è¯„ä¼°ç»“æœåˆ°æ–‡ä»¶
        self._save_evaluation_results(kurtosis_mean, mi_mean, sources.shape[0], sources.shape[1])

    def _save_evaluation_results(self, kurtosis_mean, mi_mean, n_components, n_samples):
        """æŒç»­ä¿å­˜ORICAè¯„ä¼°ç»“æœåˆ°æ–‡ä»¶"""
        timestamp = datetime.now().isoformat()
        
        # å‡†å¤‡æ•°æ®
        evaluation_data = {
            'timestamp': timestamp,
            'kurtosis_mean': float(kurtosis_mean),
            'mi_mean': float(mi_mean),
            'n_components': n_components,
            'n_samples': n_samples
        }
        
        # ç¡®ä¿Resultsç›®å½•å­˜åœ¨
        os.makedirs('./Results', exist_ok=True)
        
        # ä¿å­˜åˆ°JSONæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        json_filename = './Results/orica_evaluation_continuous.json'
        self._append_to_json(json_filename, evaluation_data)
        
        # ä¿å­˜åˆ°CSVæ–‡ä»¶ï¼ˆè¿½åŠ æ¨¡å¼ï¼‰
        csv_filename = './Results/orica_evaluation_continuous.csv'
        self._append_to_csv(csv_filename, evaluation_data)
        
        print(f"âœ… è¯„ä¼°ç»“æœå·²ä¿å­˜: kurtosis_mean={kurtosis_mean:.3f}, mi_mean={mi_mean:.3f}")

    def _append_to_json(self, filename, data):
        """å°†æ•°æ®è¿½åŠ åˆ°JSONæ–‡ä»¶"""
        try:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶
            if not os.path.exists(filename):
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump([data], f, ensure_ascii=False, indent=2)
            else:
                # è¯»å–ç°æœ‰æ•°æ®
                try:
                    with open(filename, 'r', encoding='utf-8') as f:
                        existing_data = json.load(f)
                except (json.JSONDecodeError, FileNotFoundError):
                    existing_data = []
                
                # è¿½åŠ æ–°æ•°æ®
                if isinstance(existing_data, list):
                    existing_data.append(data)
                else:
                    existing_data = [existing_data, data]
                
                # å†™å›æ–‡ä»¶
                with open(filename, 'w', encoding='utf-8') as f:
                    json.dump(existing_data, f, ensure_ascii=False, indent=2)
                    
        except Exception as e:
            print(f"âŒ ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")

    def _append_to_csv(self, filename, data):
        """å°†æ•°æ®è¿½åŠ åˆ°CSVæ–‡ä»¶"""
        try:
            # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œåˆ›å»ºæ–°æ–‡ä»¶å¹¶å†™å…¥è¡¨å¤´
            if not os.path.exists(filename):
                with open(filename, 'w', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=data.keys())
                    writer.writeheader()
                    writer.writerow(data)
            else:
                # è¿½åŠ æ•°æ®
                with open(filename, 'a', newline='', encoding='utf-8') as f:
                    writer = csv.DictWriter(f, fieldnames=data.keys())
                    writer.writerow(data)
                    
        except Exception as e:
            print(f"âŒ ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")

    def evaluate_orica_sourcesx(self,sources, n_bins=10):
        from scipy.stats import kurtosis
        from sklearn.metrics import mutual_info_score
        import numpy as np

        # å³­åº¦
        kurt_vals = kurtosis(sources, axis=1, fisher=True)
        # print("ã€ORICAè¯„ä¼°ã€‘å„ICçš„å³­åº¦ï¼ˆè¶Šå¤§è¶Šéé«˜æ–¯ï¼Œåˆ†ç¦»æ•ˆæœè¶Šå¥½ï¼‰:")
        # print(kurt_vals)
        print(f"kurtosis mean{np.mean(np.abs(kurt_vals)):.3f}")

        # äº’ä¿¡æ¯ï¼ˆå…ˆç¦»æ•£åŒ–ï¼‰
        n = sources.shape[0]
        mi_matrix = np.zeros((n, n))
        # ç¦»æ•£åŒ–
        digitized = np.array([np.digitize(s, np.histogram(s, bins=n_bins)[1]) for s in sources])
        for i in range(n):
            for j in range(n):
                if i != j:
                    mi_matrix[i, j] = mutual_info_score(digitized[i], digitized[j])
        #print("ã€ORICAè¯„ä¼°ã€‘ICä¹‹é—´çš„äº’ä¿¡æ¯çŸ©é˜µï¼ˆè¶Šæ¥è¿‘0è¶Šç‹¬ç«‹ï¼‰:")
        np.set_printoptions(precision=3, suppress=True)
        #print(mi_matrix)
        # åªç»Ÿè®¡éå¯¹è§’çº¿å…ƒç´ çš„å‡å€¼
        mi_mean = np.sum(mi_matrix) / (n * (n - 1))
        print(f"MI meanï¼š{mi_mean:.3f}")



    def fit(self, data, channel_range, chan_labels, srate):
        """Fit ICA on the buffered EEG data
        è¿”å›ï¼š
            sources: ICAåˆ†ç¦»å‡ºçš„ç‹¬ç«‹æˆåˆ†ä¿¡å·ï¼ˆcomponents, samplesï¼‰
            A: ICA mixing matrix (é€šé“æ•°, æˆåˆ†æ•°)
            spectrum: dictï¼ŒåŒ…å«æ‰€æœ‰ICåˆ†é‡çš„é¢‘è°±ï¼ˆ'freqs': é¢‘ç‡, 'powers': shape=(n_components, n_freqs)ï¼‰
        """

        assert data.shape[0] == self.n_components, f"Expected {self.n_components} channels, got {data.shape[0]}"
        #print("data.shape[1]",data.shape[1])
        if data.shape[1] < self.max_samples:
            return None, None, None  # Not enough data yet







        # self.ica = ORICA(n_components=min(self.n_components, data.shape[0]), learning_rate=0.001)
        # #self.ica = ORICA(n_components=min(self.n_components, data.shape[0]), learning_rate=0.001)
        # sources = self.ica.fit_transform(data.T).T  # shape: (components, samples)


        # self.ica = ORICA(n_components=min(self.n_components, data.shape[0]))
        # self.ica.initialize(data.T)  # only once, when data is available

        # # åœ¨æ•°æ®æµä¸­é€ä¸ªè¾“å…¥æ ·æœ¬ï¼ˆæˆ–è€…å°æ‰¹æ¬¡ï¼‰
        # sources = np.array([self.ica.partial_fit(x_t) for x_t in data.T]).T



        # åªåœ¨ç¬¬ä¸€æ¬¡åˆ›å»ºORICAå®ä¾‹ï¼Œé¿å…é‡å¤åˆå§‹åŒ–
        if self.ica is None:
            print("ğŸ”„ é¦–æ¬¡åˆ›å»ºORICAå®ä¾‹")
            self.ica = ORICA_final(n_components=min(self.n_components, data.shape[0]))
            self.ica.initialize(data.T)
            sources,x,y = self.ica.fit(data.T)
            # print("sources1",sources.shape)#(22,5000)
            # print("srate1",self.srate)#500
            #
        else:
            sources,x,y = self.ica.fit(data.T)
            # print("sources",sources.shape)#(22,5000)
            # print("srate",self.srate)#500

        self.evaluate_orica_sources(sources)



        
        '''
            self.ica = ORICAZ(n_components=min(self.n_components, data.shape[0]))
            self.ica.initialize(data.T)  # åªåœ¨ç¬¬ä¸€æ¬¡åˆå§‹åŒ–
            #print("test",data.T.shape)#test (2500, 25)
            # é¦–æ¬¡åˆå§‹åŒ–åï¼Œä½¿ç”¨partial_fitå¤„ç†æ•°æ®
            #x_t (25,)

            #sources = np.array([self.ica.partial_fit(x_t) for x_t in data.T]).T
            # ä½¿ç”¨fit_online_streamå¤„ç†æ•´ä¸ªæ•°æ®æµ
            #sources = self.ica.fit_online_stream(data.T)
            sources = self.ica.fit_block_stream(data.T)


            # #ç”¨äºæµ‹è¯•rlsæœ‰æ— çš„å·®åˆ«
            # self.icax = ORICA1(n_components=min(self.n_components, data.shape[0]))
            # self.icax.initialize(data.T)  # åªåœ¨ç¬¬ä¸€æ¬¡åˆå§‹åŒ–
            # #print("test",data.T.shape)#test (2500, 25)
            # # é¦–æ¬¡åˆå§‹åŒ–åï¼Œä½¿ç”¨partial_fitå¤„ç†æ•°æ®
            # #x_t (25,)
            # sourcesx = np.array([self.icax.partial_fit(x_t) for x_t in data.T]).T

        else:
            #print("ğŸ“ˆ ç»§ç»­ä½¿ç”¨ç°æœ‰ORICAå®ä¾‹è¿›è¡Œå­¦ä¹ ")
            # ç›´æ¥ä½¿ç”¨partial_fitè¿›è¡Œåœ¨çº¿å­¦ä¹ ï¼Œä¸é‡æ–°åˆå§‹åŒ–

            #sources = np.array([self.ica.partial_fit(x_t) for x_t in data.T]).T#sources (25, 5000)
            # ä½¿ç”¨fit_online_streamå¤„ç†æ•´ä¸ªæ•°æ®æµ
            #sources = self.ica.fit_online_stream(data.T)#sources (5000, 25)
            sources = self.ica.fit_block_stream(data.T)

            #sourcesx = np.array([self.icax.partial_fit(x_t) for x_t in data.T]).T
            # for x_t in data.T:
            #     self.ica.partial_fit(x_t)
        # print("-"*50)
        #print("sources",sources.shape)
        #self.evaluate_orica_sources(sources)
        
        #self.evaluate_orica_sources(sourcesx)
        print("sources",sources.shape)
        print("srate",self.srate)
        '''

        # è·å–åˆ†ç¦»åçš„æºä¿¡å·
        #sources = self.ica.transform(data.T).T  # shape: (components, samples)

        # å³°åº¦è¯„ä¼°
        # k = self.ica.evaluate_separation(sources)
        # np.set_printoptions(threshold=np.inf, precision=4)  # æ˜¾ç¤ºæ‰€æœ‰å…ƒç´ ï¼Œä¿ç•™4ä½å°æ•°
        # print("Kurtosis of components:", k)
        # np.set_printoptions()  # æ¢å¤é»˜è®¤è®¾ç½®

        # success=self.assess_orica_success(k)
        # if success:
        #     print("âœ… ORICA åˆ†ç¦»æˆåŠŸï¼ˆå…¨å±€åˆ¤æ–­ï¼‰")
        # else:
        #     print("âŒ ORICA åˆ†ç¦»å¤±è´¥æˆ–æ— æ˜¾è‘—éé«˜æ–¯æº")

        # æ’åºæˆåˆ†ï¼ˆé«˜éé«˜æ–¯æ€§ä¼˜å…ˆï¼‰
        # sorted_idx, k = self.ica.rank_components_by_kurtosis(sources)
        # Y_sorted = sources[:, sorted_idx]

        #print("shwo",Y_sorted)
        # mi_matrix = self.ica.calc_mutual_info_matrix(sources)
        # np.set_printoptions(threshold=np.inf, precision=4)  # æ˜¾ç¤ºæ‰€æœ‰å…ƒç´ ï¼Œä¿ç•™4ä½å°æ•°
        # print("äº’ä¿¡æ¯çŸ©é˜µ:")
        # print(mi_matrix)
        # print("æœ€å¤§äº’ä¿¡æ¯:", np.max(mi_matrix))
        # print("å¹³å‡äº’ä¿¡æ¯:", np.mean(mi_matrix))
        # np.set_printoptions()  # æ¢å¤é»˜è®¤è®¾ç½®


        #ic_probs, ic_labels = self.classify(data[channel_range, :],chan_labels, srate)
        #ic_probs, ic_labels = self.classify_with_mne_ica(data[channel_range, :],chan_labels, srate)
        
        #if ic_probs is not None and ic_labels is not None:
        #     print('ICLabelæ¦‚ç‡:', ic_probs)
        #     print('ICLabelæ ‡ç­¾:', ic_labels)

        self.identify_eye_artifacts(sources, self.srate)
        #self.identify_artifacts_by_iclabel(ic_labels, ic_probs, threshold=0.8)
        

        # è·å–mixing matrix A
        try:
            A = np.linalg.pinv(self.ica.W)
        except Exception:
            A = None


        # è·å–æ‰€æœ‰ICåˆ†é‡çš„spectrum
        spectrum = None
        if sources is not None and self.srate is not None:
            powers = []
            freqs = None
            for ic in range(sources.shape[0]):
                f, Pxx = welch(sources[ic], fs=float(self.srate))
                if freqs is None:
                    freqs = f
                powers.append(Pxx)
            powers = np.array(powers)  # shape: (n_components, n_freqs)
            spectrum = {'freqs': freqs, 'powers': powers}

        #print("2")
        # --- ICèƒ½é‡æ’åº ---
        if sources is not None:
            # è®¡ç®—æ¯ä¸ªICçš„ä½é¢‘å æ¯”
            ratios = []
            for comp in sources:
                fft_vals = np.abs(np.fft.rfft(comp))
                freqs = np.fft.rfftfreq(comp.shape[0], 1 / self.srate)
                low_freq_power = np.sum(fft_vals[(freqs >= 0.1) & (freqs <= 4)])
                total_power = np.sum(fft_vals)
                ratio = low_freq_power / (total_power + 1e-10)
                ratios.append(ratio)
            ratios = np.array(ratios)
            # æŒ‰ä½é¢‘å æ¯”ä»å¤§åˆ°å°æ’åº
            self.sorted_idx = np.argsort(-ratios)
            sources = sources[self.sorted_idx, :]
            #print("xxxxx",self.sorted_idx)
            if A is not None:
                A = A[:, self.sorted_idx]
            # å¯¹Wæ’åºå¹¶ä¿å­˜
            #print("1")
            if hasattr(self.ica, 'get_W'):
                W = self.ica.get_W()
                self.sorted_W = W[self.sorted_idx, :]



        return sources, A, spectrum

    # def identify_eye_artifacts(self, components):
    #     """Heuristic: identify eye components as those with high frontal power and low frequency"""
    #     self.eog_indices = []
    #     for i, comp in enumerate(components):
    #         power = np.sum(comp ** 2)
    #         if power > np.percentile([np.sum(c ** 2) for c in components], 90):
    #             self.eog_indices.append(i)
    #
    #     print("EOG artifact:",self.eog_indices)




    def classify(self, data, chan_names, srate, montage='standard_1020'):
        """
        ç”¨ mne-icalabel å¯¹å½“å‰çª—å£çš„ICAç»“æœè¿›è¡Œåˆ†ç±»ã€‚
        è¾“å…¥:
            data: shape=(n_channels, n_samples)ï¼ŒåŸå§‹EEGçª—å£æ•°æ®
            chan_names: é€šé“ålist
            srate: é‡‡æ ·ç‡
            montage: ç”µæå¸ƒå±€
        è¾“å‡º:
            ic_probs: shape=(n_components, 7)ï¼Œæ¯ä¸ªICå±äºå„ç±»åˆ«çš„æ¦‚ç‡
            ic_labels: shape=(n_components,)ï¼Œæ¯ä¸ªICçš„ç±»åˆ«æ ‡ç­¾
        """



        # 1. æ„é€ Rawå¯¹è±¡
        info = mne.create_info(chan_names, srate, ch_types='eeg')
        raw = mne.io.RawArray(data, info)
        raw.set_montage(montage)
        raw.set_eeg_reference('average', projection=False)
        raw.filter(1., 100., fir_design='firwin')

        # 2. ç”¨ORICAåˆ†ç¦»ç»“æœä¼ªé€ ICAå¯¹è±¡
        # A: mixing matrix (n_channels, n_components)
        # W: unmixing matrix (n_components, n_channels)
        W = self.ica.get_W()  # (n_components, n_channels)
        A = np.linalg.pinv(W) # (n_channels, n_components)
        n_components = W.shape[0]
        n_channels = A.shape[0]

        xica = ICA(n_components=n_components, fit_params=dict(extended=True), method='infomax', random_state=97, max_iter='auto')
        xica.current_fit = 'ica'
        xica.n_components_ = n_components
        xica.mixing_matrix_ = A
        xica.unmixing_matrix_ = W
        setattr(xica, 'pca_explained_variance_', np.ones(n_components))
        setattr(xica, 'pca_mean_', np.zeros(n_channels))
        setattr(xica, 'pca_components_', np.eye(n_components, n_channels))

        # 3. è°ƒç”¨ICLabel
        labels = label_components(raw, xica, method='iclabel')
        print("ICLabelè¿”å›å†…å®¹ï¼š", labels)
        ic_probs = labels.get('y_pred_proba', None)
        ic_labels = labels.get('y_pred', None)
        if ic_labels is None and 'labels' in labels:
            ic_labels = labels['labels']
        return ic_probs, ic_labels


    def classify_with_mne_ica(self, data, chan_names, srate, montage='standard_1020'):
        """
        ç”¨MNEè‡ªå¸¦çš„ICAåˆ†è§£+ICLabelåˆ†ç±»ï¼Œä¾¿äºå’ŒORICA hackç»“æœå¯¹æ¯”ã€‚
        è¾“å…¥:
            data: shape=(n_channels, n_samples)
            chan_names: é€šé“ålist
            srate: é‡‡æ ·ç‡
            montage: ç”µæå¸ƒå±€
        è¾“å‡º:
            ic_probs: shape=(n_components, 7)
            ic_labels: shape=(n_components,)
        """
        import mne
        from mne.preprocessing import ICA
        from mne_icalabel import label_components

        info = mne.create_info(chan_names, srate, ch_types='eeg')
        raw = mne.io.RawArray(data, info)
        raw.set_montage(montage)
        raw.set_eeg_reference('average', projection=False)
        raw.filter(1., 100., fir_design='firwin')

        # ç”¨MNEè‡ªå¸¦ICAåˆ†è§£
        ica = ICA(n_components=data.shape[0], fit_params=dict(extended=True), method='infomax', random_state=97, max_iter='auto')
        ica.fit(raw)

        # ICLabelåˆ†ç±»
        labels = label_components(raw, ica, method='iclabel')
        print("[MNE ICA] ICLabelè¿”å›å†…å®¹ï¼š", labels)
        ic_probs = labels.get('y_pred_proba', None)
        ic_labels = labels.get('y_pred', None)
        if ic_labels is None and 'labels' in labels:
            ic_labels = labels['labels']
        return ic_probs, ic_labels



    def identify_eye_artifacts(self, components, srate):


        self.eog_indices = []


        for i, comp in enumerate(components):

            fft_vals = np.abs(np.fft.rfft(comp))

            freqs = np.fft.rfftfreq(comp.shape[0], 1 / srate)
            #print("fft_vals",fft_vals.shape)
            #print("freqs",freqs.shape)
            low_freq_power = np.sum(fft_vals[(freqs >= 0.1) & (freqs <= 4)])#0.1-4hzçš„ä½é¢‘ä¿¡å·

            total_power = np.sum(fft_vals)

            ratio = low_freq_power / (total_power + 1e-10)



            if ratio > 0.35:  # å¦‚æœä½é¢‘å æ¯”è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºæ˜¯ EOG
                self.eog_indices.append(i)
        #print(self.eog_indices)

        #print("sh:",self.eog_indices)

        #print("EOG artifact indices (low-freq based):", self.eog_indices)

    def identify_artifacts_by_iclabel(self, ic_labels, ic_probs, threshold=0.8):
        """
        æ ¹æ®ICLabelåˆ†ç±»ç»“æœè‡ªåŠ¨è¯†åˆ«ä¼ªå½±ICã€‚
        åªè¦ä¸æ˜¯'brain'ï¼Œä¸”æ¦‚ç‡å¤§äºé˜ˆå€¼ï¼ˆé»˜è®¤0.8ï¼‰ï¼Œå°±åŠ å…¥self.artifactã€‚
        """
        self.artifact = []
        for i, (label, prob) in enumerate(zip(ic_labels, ic_probs)):
            if label == 'brain':
                continue
            if label == 'other':
                if prob > threshold:
                    self.artifact.append(i)
            else:
                self.artifact.append(i)

            # if label != 'brain' and prob > threshold:
            #     self.artifact.append(i)

        #print("shitinggggggggggggggg:",self.artifact)



    def transform(self, new_data):
        #å»é™¤ä¼ªå½±çš„ç‹¬ç«‹æˆåˆ†åé‡æ–°æ˜ å°„å›åŸæ¥çš„é€šé“ã€‚
        if self.ica is None:
            return new_data

        #print("x")
        sources = self.ica.transform(new_data.T)
        sources[:, self.eog_indices] = 0  # Zero out EOG components
        #sources[:, self.eog_indices] = 0  # Zero out EOG components
        cleaned = self.ica.inverse_transform(sources)
        #print("y")
        return cleaned.T

    def update_buffer(self, new_chunk):
        if self.data_buffer is None:
            self.data_buffer = new_chunk
        else:
            self.data_buffer = np.concatenate([self.data_buffer, new_chunk], axis=1)
            #å½“æ•°æ®è¶³å¤Ÿçš„æ—¶å€™æ‰ç”Ÿæˆself.data_buffer,ä¹‹åä¸€ç›´ä¿æŒ1500çš„é•¿åº¦ï¼Œå¹¶ä¸”ä¸æ–­ç§»åŠ¨çª—å£
            if self.data_buffer.shape[1] > self.max_samples:
                self.data_buffer = self.data_buffer[:, -self.max_samples:]

        #print("data_buffer")
        #print(np.array(self.data_buffer).shape)

        return self.data_buffer.shape[1] >= self.max_samples
        #è¿™ä¸€è¡Œå°±æ˜¯ä¸ºå•¥oricaéœ€è¦ç­‰ä¸€æ®µæ—¶é—´ï¼Œå› ä¸ºæˆ‘éœ€è¦ç­‰åˆ°è¶³å¤Ÿæ•°æ®åï¼Œæ‰èƒ½
        #åœ¨stream_receiver.py,
        #if self.orica.update_buffer(chunk[self.channel_range, :]):
        #è¿™å¥è¯åˆ¤æ–­ä¸ºtrue