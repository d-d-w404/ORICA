#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¢å¼ºç‰ˆORICA - è§£å†³ICé›†ä¸­åœ¨channelçš„é—®é¢˜
"""

import numpy as np
from scipy.stats import kurtosis
from sklearn.feature_selection import mutual_info_regression

class ORICAW:
    def __init__(self, n_components, learning_rate=0.01, ortho_every=10, 
                 use_rls_whitening=True, forgetting_factor=0.98, 
                 nonlinearity='gaussian', enhanced_init=True):
        """
        å¢å¼ºç‰ˆORICA
        
        Args:
            n_components: ç‹¬ç«‹æˆåˆ†æ•°é‡
            learning_rate: å­¦ä¹ ç‡
            ortho_every: æ­£äº¤åŒ–é¢‘ç‡
            use_rls_whitening: æ˜¯å¦ä½¿ç”¨RLSç™½åŒ–
            forgetting_factor: RLSé—å¿˜å› å­
            nonlinearity: éçº¿æ€§å‡½æ•°ç±»å‹
            enhanced_init: æ˜¯å¦ä½¿ç”¨å¢å¼ºåˆå§‹åŒ–
        """
        self.n_components = n_components
        self.learning_rate = learning_rate
        self.ortho_every = ortho_every
        self.W = np.eye(n_components)
        self.mean = None
        self.whitening_matrix = None
        self.whitened = False
        self.update_count = 0
        
        # RLSç™½åŒ–å‚æ•°
        self.use_rls_whitening = use_rls_whitening
        self.forgetting_factor = forgetting_factor
        self.nonlinearity = nonlinearity
        
        # å¢å¼ºåŠŸèƒ½å‚æ•°
        self.enhanced_init = enhanced_init
        self.spatial_diversity_weight = 0.1  # ç©ºé—´å¤šæ ·æ€§æƒé‡
        self.entropy_weight = 0.05  # ç†µæƒé‡
        
        if self.use_rls_whitening:
            self.C = None
            self.t = 0
        
        # æ€§èƒ½ç›‘æ§
        self.spatial_concentration_history = []
        self.entropy_history = []
        
    def _enhanced_initialization(self, X_init):
        """å¢å¼ºåˆå§‹åŒ–ç­–ç•¥"""
        print("ğŸ”§ ä½¿ç”¨å¢å¼ºåˆå§‹åŒ–ç­–ç•¥...")
        
        n_samples, n_channels = X_init.shape
        
        # 1. åŸºäºPCAçš„åˆå§‹åŒ–
        # è®¡ç®—åæ–¹å·®çŸ©é˜µ
        cov_matrix = np.cov(X_init.T)
        
        # ç‰¹å¾å€¼åˆ†è§£
        eigenvals, eigenvecs = np.linalg.eigh(cov_matrix)
        
        # é€‰æ‹©å‰n_componentsä¸ªä¸»æˆåˆ†
        sorted_indices = np.argsort(eigenvals)[::-1]
        selected_indices = sorted_indices[:self.n_components]
        
        # åˆå§‹åŒ–WçŸ©é˜µä¸ºä¸»æˆåˆ†æ–¹å‘
        W_pca = eigenvecs[:, selected_indices].T
        
        # 2. æ·»åŠ éšæœºæ‰°åŠ¨ä»¥å¢åŠ å¤šæ ·æ€§
        noise = np.random.randn(*W_pca.shape) * 0.1
        W_enhanced = W_pca + noise
        
        # 3. æ­£äº¤åŒ–
        U, _, Vt = np.linalg.svd(W_enhanced)
        self.W = U @ Vt
        
        print(f"âœ… å¢å¼ºåˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨PCA + éšæœºæ‰°åŠ¨ç­–ç•¥")
        
    def _compute_spatial_diversity(self):
        """è®¡ç®—ç©ºé—´å¤šæ ·æ€§"""
        # è®¡ç®—WçŸ©é˜µçš„æ¡ä»¶æ•°
        condition_number = np.linalg.cond(self.W)
        
        # è®¡ç®—WçŸ©é˜µçš„å¥‡å¼‚å€¼åˆ†å¸ƒ
        singular_values = np.linalg.svd(self.W, compute_uv=False)
        sv_ratio = np.min(singular_values) / np.max(singular_values)
        
        # è®¡ç®—ç©ºé—´é›†ä¸­åº¦
        spatial_concentration = np.mean(np.max(np.abs(self.W), axis=1) / np.mean(np.abs(self.W), axis=1))
        
        return {
            'condition_number': condition_number,
            'sv_ratio': sv_ratio,
            'spatial_concentration': spatial_concentration
        }
    
    def _compute_entropy(self, y_t):
        """è®¡ç®—ç†µ"""
        # ä½¿ç”¨ç›´æ–¹å›¾ä¼°è®¡ç†µ
        hist, _ = np.histogram(y_t, bins=20, density=True)
        hist = hist[hist > 0]  # ç§»é™¤é›¶æ¦‚ç‡
        entropy = -np.sum(hist * np.log(hist))
        return entropy
    
    def _spatial_diversity_loss(self):
        """ç©ºé—´å¤šæ ·æ€§æŸå¤±"""
        # é¼“åŠ±WçŸ©é˜µçš„åˆ—å‘é‡æ›´åŠ åˆ†æ•£
        W_normalized = self.W / np.linalg.norm(self.W, axis=1, keepdims=True)
        
        # è®¡ç®—åˆ—å‘é‡é—´çš„ç›¸ä¼¼åº¦
        similarity_matrix = W_normalized @ W_normalized.T
        
        # å¯¹è§’çº¿å…ƒç´ è®¾ä¸º0ï¼ˆæ’é™¤è‡ªèº«ç›¸ä¼¼åº¦ï¼‰
        np.fill_diagonal(similarity_matrix, 0)
        
        # å¤šæ ·æ€§æŸå¤±ï¼šæœ€å°åŒ–ç›¸ä¼¼åº¦
        diversity_loss = np.mean(similarity_matrix**2)
        
        return diversity_loss
    
    def _center(self, X):
        """å»å‡å€¼"""
        self.mean = np.mean(X, axis=0)
        return X - self.mean

    def _whiten(self, X):
        """æ”¹è¿›çš„ç™½åŒ–ç­–ç•¥"""
        cov = np.cov(X, rowvar=False)
        
        # æ·»åŠ æ­£åˆ™åŒ–ä»¥æé«˜æ•°å€¼ç¨³å®šæ€§
        reg_factor = 1e-6
        cov_reg = cov + reg_factor * np.eye(cov.shape[0])
        
        d, E = np.linalg.eigh(cov_reg)
        
        # ç¡®ä¿ç‰¹å¾å€¼ä¸ºæ­£
        d = np.maximum(d, 1e-8)
        
        D_inv = np.diag(1.0 / np.sqrt(d))
        self.whitening_matrix = E @ D_inv @ E.T
        
        return X @ self.whitening_matrix.T

    def _rls_whiten_initialize(self, X):
        """åˆå§‹åŒ–RLSç™½åŒ–"""
        n_channels = X.shape[1]
        self.C = np.eye(n_channels)
        self.whitening_matrix = np.eye(n_channels)
        self.t = 0

    def _rls_whiten_update(self, x_t):
        """RLSç™½åŒ–å•æ­¥æ›´æ–°"""
        if self.C is None:
            raise ValueError("RLS whitening not initialized")
        
        lambda_t = self.forgetting_factor
        self.t += 1
        
        # è®¡ç®—å¢ç›Šå‘é‡
        k = self.C @ x_t
        denominator = lambda_t + x_t.T @ k
        
        # æ•°å€¼ç¨³å®šæ€§æ£€æŸ¥
        if denominator < 1e-10:
            return self.whitening_matrix @ x_t
        
        k = k / denominator
        
        # æ›´æ–°åæ–¹å·®çŸ©é˜µçš„é€†
        self.C = (self.C - k @ x_t.T @ self.C) / lambda_t
        
        # ç¡®ä¿å¯¹ç§°æ€§
        self.C = (self.C + self.C.T) / 2
        
        # æ›´æ–°ç™½åŒ–çŸ©é˜µ
        try:
            eigenvals, eigenvecs = np.linalg.eigh(self.C)
            eigenvals = np.maximum(eigenvals, 1e-6)
            self.whitening_matrix = eigenvecs @ np.diag(np.sqrt(eigenvals)) @ eigenvecs.T
        except np.linalg.LinAlgError:
            pass
        
        return self.whitening_matrix @ x_t

    def _g(self, y):
        """éçº¿æ€§å‡½æ•°åŠå…¶å¯¼æ•°"""
        if self.nonlinearity == 'gaussian':
            g_y = y * np.exp(-y**2/2)
            g_prime = (1 - y**2) * np.exp(-y**2/2)
        elif self.nonlinearity == 'tanh':
            g_y = np.tanh(y)
            g_prime = 1 - np.tanh(y)**2
        else:
            raise ValueError(f"Unknown nonlinearity: {self.nonlinearity}")
        return g_y, g_prime

    def initialize(self, X_init):
        """åˆå§‹åŒ–"""
        print(f"ğŸ”§ åˆå§‹åŒ–å¢å¼ºç‰ˆORICA: æˆåˆ†æ•°={self.n_components}")
        
        X_init = self._center(X_init)
        
        # ç™½åŒ–
        if self.use_rls_whitening:
            self._rls_whiten_initialize(X_init)
            X_init = self._whiten(X_init)
        else:
            X_init = self._whiten(X_init)
        
        # å¢å¼ºåˆå§‹åŒ–
        if self.enhanced_init:
            self._enhanced_initialization(X_init)
        
        self.whitened = True
        print(f"âœ… åˆå§‹åŒ–å®Œæˆ")

    def partial_fit(self, x_t):
        """å•ä¸ªæ ·æœ¬åœ¨çº¿æ›´æ–°"""
        if not self.whitened:
            raise ValueError("Must call `initialize` with initial batch before `partial_fit`.")
        
        # å»å‡å€¼
        if self.mean is not None:
            x_t = x_t - self.mean
        
        # ç™½åŒ–
        if self.use_rls_whitening:
            x_t_whitened = self._rls_whiten_update(x_t.reshape(-1, 1)).ravel()
        else:
            x_t_whitened = self.whitening_matrix @ x_t
        
        # ICAæ›´æ–°
        y_t = self.W @ x_t_whitened
        g_y, _ = self._g(y_t)
        
        # æ ‡å‡†ORICAæ›´æ–°è§„åˆ™
        I = np.eye(self.n_components)
        delta_W_standard = self.learning_rate * ((I - g_y @ y_t.T) @ self.W)
        
        # å¢å¼ºæ›´æ–°ï¼šæ·»åŠ ç©ºé—´å¤šæ ·æ€§çº¦æŸ
        diversity_loss = self._spatial_diversity_loss()
        diversity_gradient = self._compute_diversity_gradient()
        
        # è®¡ç®—ç†µ
        entropy = self._compute_entropy(y_t)
        
        # ç»„åˆæ›´æ–°
        delta_W_enhanced = delta_W_standard + self.spatial_diversity_weight * diversity_gradient
        
        self.W += delta_W_enhanced
        
        # æ­£äº¤åŒ–
        self.update_count += 1
        if self.update_count % self.ortho_every == 0:
            U, _, Vt = np.linalg.svd(self.W)
            self.W = U @ Vt
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            spatial_metrics = self._compute_spatial_diversity()
            self.spatial_concentration_history.append(spatial_metrics['spatial_concentration'])
            self.entropy_history.append(entropy)
            
            # æ‰“å°ç›‘æ§ä¿¡æ¯
            # if self.update_count % 100 == 0:
            #     print(f"  æ­¥éª¤ {self.update_count}: ç©ºé—´é›†ä¸­åº¦={spatial_metrics['spatial_concentration']:.3f}, "
            #           f"ç†µ={entropy:.3f}, å¤šæ ·æ€§æŸå¤±={diversity_loss:.4f}")
        
        return y_t
    
    def _compute_diversity_gradient(self):
        """è®¡ç®—å¤šæ ·æ€§çº¦æŸçš„æ¢¯åº¦"""
        W_normalized = self.W / np.linalg.norm(self.W, axis=1, keepdims=True)
        
        # è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity_matrix = W_normalized @ W_normalized.T
        np.fill_diagonal(similarity_matrix, 0)
        
        # è®¡ç®—æ¢¯åº¦
        gradient = 2 * similarity_matrix @ W_normalized
        
        return gradient
    
    def get_spatial_metrics(self):
        """è·å–ç©ºé—´åˆ†å¸ƒæŒ‡æ ‡"""
        if len(self.spatial_concentration_history) == 0:
            return None
        
        return {
            'spatial_concentration': self.spatial_concentration_history[-1],
            'entropy': self.entropy_history[-1] if len(self.entropy_history) > 0 else 0,
            'concentration_trend': np.mean(self.spatial_concentration_history[-10:]) if len(self.spatial_concentration_history) >= 10 else 0
        }
    
    def evaluate_separation(self, Y):
        """è¯„ä¼°åˆ†ç¦»æ•ˆæœ"""
        return kurtosis(Y, axis=0, fisher=False)
    
    def transform(self, X):
        """å˜æ¢æ•°æ®"""
        if not self.whitened:
            raise ValueError("Model must be initialized first.")
        
        X = X - self.mean
        X_whitened = X @ self.whitening_matrix.T
        Y = (self.W @ X_whitened.T).T
        return Y
    
    def inverse_transform(self, Y):
        """é€†å˜æ¢"""
        if not self.whitened:
            raise ValueError("Model must be initialized first.")
        
        X_whitened = np.linalg.pinv(self.W) @ Y.T
        X = X_whitened.T @ np.linalg.pinv(self.whitening_matrix)
        X = X + self.mean
        return X
    
    def get_W(self):
        """è·å–è§£æ··çŸ©é˜µ"""
        return self.W.copy()
    
    def get_whitening_matrix(self):
        """è·å–ç™½åŒ–çŸ©é˜µ"""
        return self.whitening_matrix.copy()

# def test_enhanced_orica():
#     """æµ‹è¯•å¢å¼ºç‰ˆORICA"""
#     print("=== æµ‹è¯•å¢å¼ºç‰ˆORICA ===")
    
#     # ç”Ÿæˆæµ‹è¯•æ•°æ®
#     n_channels = 25
#     n_samples = 10000
    
#     # ç”Ÿæˆæ··åˆä¿¡å·
#     sources = np.random.randn(n_channels, n_samples)
    
#     # æ·»åŠ éé«˜æ–¯æˆåˆ†
#     sources[0, :] = np.sign(sources[0, :]) * np.abs(sources[0, :])**1.5
#     sources[1, :] = np.tanh(sources[1, :])
#     sources[2, :] = sources[2, :] * np.exp(-sources[2, :]**2/2)
    
#     # ç”Ÿæˆæ··åˆçŸ©é˜µ
#     A = np.random.randn(n_channels, n_channels)
#     A = A / np.linalg.norm(A, axis=0)
    
#     # æ··åˆä¿¡å·
#     X = A @ sources
    
#     # åˆ›å»ºå¢å¼ºç‰ˆORICAå®ä¾‹
#     orica = ORICAEnhanced(
#         n_components=n_channels,
#         learning_rate=0.01,
#         ortho_every=10,
#         use_rls_whitening=False,
#         enhanced_init=True
#     )
    
#     # åˆå§‹åŒ–
#     orica.initialize(X[:1000].T)
    
#     # åœ¨çº¿å­¦ä¹ 
#     performance_history = []
#     spatial_metrics_history = []
    
#     print("å¼€å§‹åœ¨çº¿å­¦ä¹ ...")
#     for i in range(1000, len(X), 100):
#         batch = X[:, i:i+100].T
        
#         # å¤„ç†æ‰¹æ¬¡
#         sources_batch = []
#         for x_t in batch:
#             result = orica.partial_fit(x_t)
#             sources_batch.append(result)
        
#         # è¯„ä¼°æ€§èƒ½
#         sources_array = np.array(sources_batch)
#         kurt_vals = kurtosis(sources_array, axis=0, fisher=True)
#         kurt_mean = np.mean(np.abs(kurt_vals))
#         performance_history.append(kurt_mean)
        
#         # è·å–ç©ºé—´æŒ‡æ ‡
#         spatial_metrics = orica.get_spatial_metrics()
#         if spatial_metrics:
#             spatial_metrics_history.append(spatial_metrics)
        
#         if len(performance_history) % 20 == 0:
#             print(f"  æ­¥éª¤ {len(performance_history)*100}: Kurtosis={kurt_mean:.3f}")
#             if spatial_metrics:
#                 print(f"    ç©ºé—´é›†ä¸­åº¦: {spatial_metrics['spatial_concentration']:.3f}")
    
#     print(f"\nâœ… å­¦ä¹ å®Œæˆ")
    
#     # åˆ†ææœ€ç»ˆç»“æœ
#     final_sources = orica.transform(X.T)
#     final_mixing_matrix = np.linalg.pinv(orica.get_W())
    
#     # ä½¿ç”¨åˆ†æå·¥å…·
#     from analyze_ica_separation import analyze_ica_separation_quality
#     analyze_ica_separation_quality(final_sources.T, final_mixing_matrix, 
#                                  [f'Ch{i}' for i in range(n_channels)])
    
#     # ç»˜åˆ¶æ€§èƒ½å˜åŒ–
#     import matplotlib.pyplot as plt
    
#     plt.figure(figsize=(15, 5))
    
#     # æ€§èƒ½å˜åŒ–
#     plt.subplot(1, 3, 1)
#     plt.plot(performance_history, label='Kurtosis', linewidth=2)
#     plt.title('å¢å¼ºORICAæ€§èƒ½å˜åŒ–')
#     plt.xlabel('æ›´æ–°æ­¥éª¤ (x100)')
#     plt.ylabel('Kurtosis Mean')
#     plt.legend()
#     plt.grid(True)
    
#     # ç©ºé—´é›†ä¸­åº¦å˜åŒ–
#     if spatial_metrics_history:
#         plt.subplot(1, 3, 2)
#         concentrations = [m['spatial_concentration'] for m in spatial_metrics_history]
#         plt.plot(concentrations, label='ç©ºé—´é›†ä¸­åº¦', linewidth=2)
#         plt.title('ç©ºé—´é›†ä¸­åº¦å˜åŒ–')
#         plt.xlabel('æ›´æ–°æ­¥éª¤ (x100)')
#         plt.ylabel('ç©ºé—´é›†ä¸­åº¦')
#         plt.legend()
#         plt.grid(True)
        
#         # ç†µå˜åŒ–
#         plt.subplot(1, 3, 3)
#         entropies = [m['entropy'] for m in spatial_metrics_history]
#         plt.plot(entropies, label='ç†µ', linewidth=2)
#         plt.title('ç†µå˜åŒ–')
#         plt.xlabel('æ›´æ–°æ­¥éª¤ (x100)')
#         plt.ylabel('ç†µ')
#         plt.legend()
#         plt.grid(True)
    
#     plt.tight_layout()
#     plt.savefig('enhanced_orica_performance.png', dpi=300, bbox_inches='tight')
#     plt.show()

